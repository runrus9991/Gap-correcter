{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Space correction.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "MjUaPbYoyei1"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AF-nz19_NGDP",
        "colab_type": "text"
      },
      "source": [
        "## Импорт библиотек"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQ0K8JPCHJb2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "4d2afb79-8146-4d9c-d279-2173bb517eb4"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding, Bidirectional, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from random import uniform\n",
        "from collections import defaultdict\n",
        "import requests\n",
        "import codecs\n",
        "import random\n",
        "import networkx as nx\n",
        "import math\n",
        "\n",
        "!pip install pymystem3==0.1.10\n",
        "import json\n",
        "from pymystem3 import Mystem\n",
        "\n",
        "m = Mystem()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymystem3==0.1.10 in /usr/local/lib/python3.6/dist-packages (0.1.10)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pymystem3==0.1.10) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pymystem3==0.1.10) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pymystem3==0.1.10) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pymystem3==0.1.10) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pymystem3==0.1.10) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icVWpxKWNOBc",
        "colab_type": "text"
      },
      "source": [
        "## Обучение"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0V4qWSRHmB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## АЛГОРИТМ АХО-КОРАСИКА ------------\n",
        "\n",
        "def graph_construction(list_of_words, sentence_len):\n",
        "    '''\n",
        "    Построение графа всех возможных разбиений.\n",
        "    '''\n",
        "    sentence_len += 2\n",
        "\n",
        "    G=nx.MultiDiGraph()\n",
        "    G.add_nodes_from(range(0, sentence_len))\n",
        "    G.number_of_nodes()\n",
        "    base_for_graph = []\n",
        "    \n",
        "    for i in range(0, len(list_of_words)) :\n",
        "        base_for_graph.append((list_of_words[i][1], str(i) + '_word'))\n",
        "        base_for_graph.append((str(i) + '_word', list_of_words[i][2]))\n",
        "\n",
        "    G.add_edges_from(base_for_graph)\n",
        "\n",
        "    return [[list_of_words[int(word.split('_')[0])][0] for word in word if type(word) != int] \\\n",
        "        for word in nx.all_simple_paths(G, 0, sentence_len - 2)]\n",
        "\n",
        "\n",
        "def edits1(word):\n",
        "    '''\n",
        "    Функция поиска всех возможных ближайших слов\n",
        "    (с наименьшим количеством перестановок)\n",
        "    модель Левенштейна\n",
        "    '''\n",
        "    n = len(word)\n",
        "    return list( [word[0:i]+word[i+1:] for i in range(n)] +                       # удаление символа\n",
        "                [word[0:i]+word[i+1]+word[i]+word[i+2:] for i in range(n-1)] +   # транспозиция соседних символов\n",
        "                [word[0:i]+c+word[i+1:] for i in range(n) for c in alphabet] +   # замена символа\n",
        "                [word[0:i]+c+word[i:] for i in range(n+1) for c in alphabet])   # вставка лишнего символа\n",
        "\n",
        "\n",
        "class AhoNode:\n",
        "# Класс для построения дерева\n",
        "\n",
        "    def __init__(self):\n",
        "        self.goto = {}\n",
        "        self.out = []\n",
        "        self.fail = None\n",
        "\n",
        "\n",
        "def aho_create_forest(patterns):\n",
        "    '''\n",
        "    Создать бор - дерево паттернов.\n",
        "    '''\n",
        "    root = AhoNode()\n",
        "\n",
        "    for path in patterns:\n",
        "        node = root\n",
        "        for symbol in path:\n",
        "            node = node.goto.setdefault(symbol, AhoNode())\n",
        "        node.out.append(path)\n",
        "    return root\n",
        "\n",
        "\n",
        "def aho_create_statemachine(patterns):\n",
        "    '''\n",
        "    Непосредственно содание автомата Ахо-Корасика.\n",
        "    '''\n",
        "    root = aho_create_forest(patterns)\n",
        "#     print(root[0])\n",
        "    queue = []\n",
        "    for node in root.goto.values():\n",
        "        queue.append(node)\n",
        "        node.fail = root\n",
        "\n",
        "        \n",
        "    while len(queue) > 0:\n",
        "        rnode = queue.pop(0)\n",
        "\n",
        "        for key, unode in rnode.goto.items():\n",
        "            queue.append(unode)\n",
        "            fnode = rnode.fail\n",
        "            while fnode is not None and key not in fnode.goto:\n",
        "                fnode = fnode.fail\n",
        "            unode.fail = fnode.goto[key] if fnode else root\n",
        "            unode.out += unode.fail.out\n",
        "\n",
        "    return root\n",
        "\n",
        "\n",
        "def aho_find_all(s, root):\n",
        "    '''\n",
        "    Находит все возможные подстроки из набора узлов в строке.\n",
        "    '''\n",
        "    founded_words = []\n",
        "    node = root\n",
        "\n",
        "    for i in range(len(s)):\n",
        "        while node is not None and s[i] not in node.goto:\n",
        "            node = node.fail\n",
        "        if node is None:\n",
        "            node = root\n",
        "            continue\n",
        "        node = node.goto[s[i]]\n",
        "\n",
        "        for pattern in node.out:\n",
        "             founded_words.append((pattern, i - len(pattern) + 1, i + 1))\n",
        "    return founded_words\n",
        "\n",
        "def my_algorithm_start(string_of_letters, root):\n",
        "    '''\n",
        "    Запуск алгоритма.\n",
        "    '''\n",
        "    return merge_all_lists([aho_find_all(new_string, root) for new_string in [string_of_letters]])\n",
        "#     return aho_find_all(string_of_letters, root)\n",
        "\n",
        "def merge_all_lists(lstlst):\n",
        "    all_lists=[]\n",
        "    for lst in lstlst:\n",
        "        all_lists.extend(lst)\n",
        "    return all_lists\n",
        "\n",
        "\n",
        "\n",
        "## ФУНКЦИЯ ИСПРАВЛЕНИЯ ОШИБОК \n",
        "\n",
        "## ФУНКЦИИ ДЛЯ ПРЕДОБРАБОТКИ СТРОК ---------------\n",
        "\n",
        "def test_data_sep(data):\n",
        "  return data[1500 : -1500], np.hstack((data[:1500], data[-1500:]))\n",
        "\n",
        "\n",
        "def is_in(list_, vall):\n",
        "    return vall in list(map(lambda x:x[0], list_))\n",
        "\n",
        "\n",
        "def max_val(mass):\n",
        "  '''\n",
        "  Возвращает индекс последнего вхождения максимального\n",
        "  элемента в список.\n",
        "  '''\n",
        "  return next(i for i in range(len(mass)-1, -1, -1) if mass[i] == max(mass))\n",
        "\n",
        "\n",
        "def nearby_check_2(words):\n",
        "  '''\n",
        "  Проверка существуют ли слова получаемые объединением с \n",
        "  соседними словами и сущесттвует ли такое слово само.\n",
        "  '''\n",
        "  len_ = len(words) + 1\n",
        "\n",
        "  if words[0] != '':\n",
        "    return [is_in(my_algorithm_start(''.join(words[:i]), root), ''.join(words[:i])) for i in range(1, len_)] + \\\n",
        "      [False for i in range(len_, 6)]\n",
        "  else: \n",
        "    return [False]\n",
        "\n",
        "\n",
        "def min_list(lists):\n",
        "  min_l = []\n",
        "  min_len =  math.inf\n",
        "\n",
        "  for list_ in lists:\n",
        "    if len(list_) < min_len:\n",
        "      min_l = []\n",
        "      min_len = len(list_)\n",
        "      min_l.append(list_)\n",
        "    elif len(list_) == min_len:\n",
        "      min_l.append(list_)\n",
        "\n",
        "  if len(min_l) == 1:\n",
        "    return min_l[0]\n",
        "\n",
        "  else:\n",
        "    return answer_func([], min_l, [])\n",
        "\n",
        "\n",
        "def answer_func(left_cont, list_of_options, right_cont):\n",
        "  probabilities = []\n",
        "  \n",
        "  for sent in list_of_options:\n",
        "    count = 0\n",
        "    prob = 0\n",
        "    sent_ = left_cont + sent + right_cont\n",
        "\n",
        "    while len(sent_) > 0:\n",
        "      paded_sent, sent_ = sent_[:12], sent_[12:]\n",
        "      count += 1\n",
        "      prob += model.predict(pad_sequences(np.array(tokenizer.texts_to_sequences([' '.join(paded_sent)])), maxlen = 12))\n",
        "\n",
        "    probabilities.append(prob[0][0]/count)\n",
        "\n",
        "  return list_of_options[max_val(probabilities)]\n",
        "\n",
        "\n",
        "def correction(part_of_sent, j, count_of_words, index):\n",
        "  '''\n",
        "  Вызов алгоритма исправления.\n",
        "  '''\n",
        "  max_len = count_of_words - j + 2\n",
        "  list_of_fragmentation = []\n",
        "  n = 6\n",
        "  # print('j = ', j)\n",
        "\n",
        "  # print(part_of_sent)\n",
        "  if index == 0:\n",
        "    while n > 1 and list_of_fragmentation == []:\n",
        "      # print('n = ', n)\n",
        "      if j == 0:\n",
        "        merged_s = ''.join(part_of_sent[0: (max_len - 2 if n - 1 > max_len else n - 1)])\n",
        "        left_cont = []\n",
        "        right_cont = part_of_sent[(max_len - 2 if n - 1 > max_len else n - 1):]\n",
        "\n",
        "      elif j == 1:\n",
        "        merged_s = ''.join(part_of_sent[0:(max_len - 1 if n > max_len else n)])\n",
        "        left_cont = []\n",
        "        right_cont = part_of_sent[(max_len - 1 if n > max_len else n):]\n",
        "\n",
        "      else:\n",
        "        merged_s = ''.join(part_of_sent[1:(max_len if n + 1 > max_len else n + 1)])\n",
        "        left_cont = part_of_sent[:1]\n",
        "        right_cont = part_of_sent[(max_len if n + 1 > max_len else n + 1):]\n",
        "        # print(merged_s)\n",
        "\n",
        "      list_of_sent = my_algorithm_start(merged_s, root)\n",
        "\n",
        "      list_of_fragmentation = graph_construction(list_of_sent, len(merged_s))\n",
        "      n -= 1\n",
        "\n",
        "  else:\n",
        "    if j == 0:\n",
        "      merged_s = ''.join(part_of_sent[0:(max_len if index + 1 > max_len else index + 1)])\n",
        "      left_cont = []\n",
        "      right_cont = part_of_sent[(max_len if index + 1 > max_len else index + 1):]\n",
        "\n",
        "    elif j == 1:\n",
        "      merged_s = ''.join(part_of_sent[1:(max_len if index + 2 > max_len else index + 2)])\n",
        "      left_cont = part_of_sent[:1]\n",
        "      right_cont = part_of_sent[(max_len if index + 2 > max_len else index + 2):]\n",
        "\n",
        "    else:\n",
        "      merged_s = ''.join(part_of_sent[2:(max_len if index + 3 > max_len else index + 3)])\n",
        "      left_cont = part_of_sent[:2]\n",
        "      right_cont = part_of_sent[(max_len if index + 3 > max_len else index + 3):]\n",
        "\n",
        "    list_of_sent = my_algorithm_start(merged_s, root)\n",
        "    list_of_fragmentation = graph_construction(list_of_sent, len(merged_s))\n",
        "\n",
        "  if list_of_fragmentation == []:\n",
        "      return [], 0\n",
        "\n",
        "  if index == 0:\n",
        "    return left_cont + min_list(list_of_fragmentation), 5 - n\n",
        "  \n",
        "  else:\n",
        "    return left_cont + answer_func(left_cont, list_of_fragmentation, right_cont), 0\n",
        "\n",
        "\n",
        "def final_cor(text_ans, flag_dot):\n",
        "  text_ans = re.sub(\" - \", \"-\", text_ans)\n",
        "  text_ans = re.sub(\"  \", \" \", text_ans)\n",
        "  text_ans = re.sub(\"\\. \\. \\.\", \"...\", text_ans)\n",
        "  if flag_dot == 1:\n",
        "    text_ans = text_ans[:-2]\n",
        "  return text_ans\n",
        "\n",
        "\n",
        "def get_splitters(sentence):\n",
        "  '''\n",
        "  Выделяет все небуквенные символы в предложениии.\n",
        "  '''\n",
        "  not_letters = ' '.join(re.findall(split_to_part, sentence)).split(' ')\n",
        "  return [elem for elem in not_letters if elem != '']\n",
        "\n",
        "\n",
        "def get_split_sent(sentence):\n",
        "  '''\n",
        "  Преобразует предложение в список фраз отделёных\n",
        "  друг от друга разделителями.\n",
        "  '''\n",
        "  clear_sent = re.sub('><', '', re.sub('[^а-яё ]', '<>', sentence))\n",
        "  return [elem for elem in clear_sent.split('<>') if elem != '']\n",
        "\n",
        "\n",
        "\n",
        "## ЗАГРУЗКА ДАННЫХ --------------\n",
        "\n",
        "def load_data():\n",
        "  '''\n",
        "  Загрузка данных для обучения.\n",
        "  '''\n",
        "  # НЕ С НАРЕЧИЯМИ\n",
        "  word_list_1 = ['невысоко', 'немало', 'немного', 'неплохо', 'несильно', 'недорого', 'неслабо', 'нехорошо', 'непросто', 'нелегко', 'негде', 'незачем', 'некуда', \n",
        "                'неоткуда', 'невдалеке', 'невмоготу', 'невмочь', 'невпопад', 'невтерпеж', 'недаром', 'невесть', 'не высоко а', 'не мало а', 'не много а', \n",
        "                'не плохо а', 'не сильно а', 'не дорого а', 'не слабо а', 'не хорошо а', 'не просто а', 'не легко а', 'далеко не', 'вовсе не', 'отнюдь не', \n",
        "                'ничуть не', 'нисколько не', 'нисколько не', 'не вполне', 'не здесь', 'не очень', 'не полностью', 'не совсем','не там', 'не туда', 'несмотря на', \n",
        "                'невзирая на', 'не смотря', 'не взирая']\n",
        "\n",
        "\n",
        "  # ОСТАЛЬНЫЕ СЛОЖНЫЕ СЛУЧАИ СЛИТНО-РАЗДЕЛЬНОГО НАПИСАНИЯ\n",
        "  word_list_2 = ['в течение', 'в деле', 'в продолжение', 'в силу', 'в виде', 'в области', 'в связи', 'в смысле', 'по мере', 'по причине', 'в меру', 'за исключением',\n",
        "                'в отличие', 'по поводу', 'в отношении', 'иметь в виду', 'ввиду', 'навстречу', 'вслед', 'внутри', 'вроде', 'вместо', 'вследствие', 'наподобие', \n",
        "                'насчёт', 'сверх', 'впросак', 'наобум', 'насмарку', 'взаперти', 'всмятку', 'испокон', 'взатяжку', 'вброд', 'вслух', 'при том', 'что бы', 'притом',\n",
        "                'вовремя', 'вволю', 'напоказ', 'вверх', 'снизу', 'вперёд', 'вглубь', 'ввек', 'наверх', 'вплотную', 'наудалую', 'зачастую', 'наглухо', 'сгоряча', \n",
        "                'набело', 'заживо', 'вдвое', 'натрое', 'вовсю', 'почему', 'поэтому', 'затем', 'доныне', 'навсегда', 'задаром', 'вполголоса', 'вполсилы', 'во время', \n",
        "                'по двое', 'по трое', 'по одному', 'по этому', 'по тому', 'за тем', 'поэтому', 'потому', 'затем ', 'во всю ширь', 'без оглядки', 'до упора', \n",
        "                'с виду', 'под стать', 'чтобы', 'от того', 'оттого', 'при чём']\n",
        "\n",
        "\n",
        "  # СЛОВАРЬ ВСЕХ ВОЗМОЖНЫХ СЛОВ (ruscorpora)\n",
        "  !wget http://ruscorpora.ru/ngrams/1grams-3.zip\n",
        "  !unzip 1grams-3.zip\n",
        "\n",
        "\n",
        "  # СЛОВАРЬ ВСЕХ ВОЗМОЖНЫХ СЛОВ (opencorpora)\n",
        "  !wget http://opencorpora.org/files/export/ngrams/unigrams.cyr.lc.zip\n",
        "  !unzip unigrams.cyr.lc.zip\n",
        "\n",
        "\n",
        "  # КОРПУС РАЗМЕЧЕННЫХ ПРЕДЛОЖЕНИЙ (opencorpora)\n",
        "  !wget http://opencorpora.org/files/export/annot/annot.opcorpora.xml.zip\n",
        "  !unzip annot.opcorpora.xml.zip\n",
        "\n",
        "  return word_list_1, word_list_2\n",
        "\n",
        "\n",
        "\n",
        "def download_sentances(list_of_words_in):\n",
        "  '''\n",
        "  Выгрузка всех предложений содержащих указанные в \n",
        "  аргументе функции слова.\n",
        "  '''\n",
        "  lst_in = []\n",
        "\n",
        "  for word in list_of_words_in:\n",
        "    download_link = 'https://kartaslov.ru/предложения-со-словом/' + word\n",
        "\n",
        "    parsed_sait = requests.get(download_link)\n",
        "    splited_data = parsed_sait.text.split('class=\"v2-sentence-box\">\\r\\n            ')[1:]\n",
        "    for sent in splited_data:\n",
        "      \n",
        "      lst_in.append(re.sub('[-—,.–!?\\'\\\"…«»</b>&laquo&raquo;():]', '', sent.split(\"        \")[0]).lower())\n",
        "    \n",
        "  return lst_in\n",
        "\n",
        "\n",
        "\n",
        "def tokenizing(whole_list, nn):\n",
        "  '''\n",
        "  Разбивает предложение \n",
        "  whole_list:\n",
        "  '''\n",
        "  all_tekenized_sent = []\n",
        "  # tokenizer\n",
        "\n",
        "  for sentence in whole_list:\n",
        "\n",
        "      tokenized_sent = []\n",
        "      \n",
        "      word_counter = 5\n",
        "      splited_sentence = sentence.split(\" \")\n",
        "      for word in splited_sentence[3:-1]:\n",
        "\n",
        "          sent = []\n",
        "          tmp_word_counter = word_counter - 5\n",
        "\n",
        "          while tmp_word_counter - word_counter < nn - 5:\n",
        "\n",
        "              if (tmp_word_counter < 0 or tmp_word_counter > len(splited_sentence) - 1) == False:\n",
        "                  sent.append(splited_sentence[tmp_word_counter])\n",
        "\n",
        "              tmp_word_counter += 1\n",
        "\n",
        "          all_tekenized_sent.append(sent)\n",
        "          word_counter += 1\n",
        "\n",
        "  return all_tekenized_sent\n",
        "\n",
        "\n",
        "\n",
        "## НАЧАЛЬНЫЕ ЗНАЧЕНИЯ ---------------\n",
        "\n",
        "word_list_1, word_list_2 = load_data()\n",
        "\n",
        "list_of_sent_ne = download_sentances(word_list_1)\n",
        "list_of_sent_hard = download_sentances(word_list_2)\n",
        "\n",
        "split_regex = re.compile(r'[.|!|?|…]')\n",
        "list_of_wrong_sent = []\n",
        "\n",
        "# wrong_text = open(\"/content/text2.txt\", \"r\",encoding='utf-8', errors='ignore').read()\n",
        "# wrong_text_list = wrong_text.split(' ')\n",
        "# random.shuffle(wrong_text_list) \n",
        "# wrong_tok = re.sub('  ', ' ', ' '.join(wrong_text_list))\n",
        "\n",
        "# for wrong_sentence in split_regex.split(wrong_tok):\n",
        "#   list_of_wrong_sent.append(re.sub('[-—,.–!?\\'\\\"…«»():]', '', wrong_sentence).lower())\n",
        "\n",
        "\n",
        "\n",
        "## СОЗДАНИЕ АВТОМАТА -------------\n",
        "\n",
        "NWORDS_df = pd.read_csv('1grams-3.txt', '\\t', names = [\"частота\", \"слово\"])\n",
        "\n",
        "NWORDS_df = NWORDS_df.loc[NWORDS_df[\"слово\"].str.len() > 6]\n",
        "NWORDS_1 = NWORDS_df[\"слово\"].tolist()\n",
        "NWORDS_1 = [word for word in NWORDS_1 if type(word) == str]\n",
        "NWORDS_1 = [word for word in NWORDS_1 if re.findall('[A-Za-z0-9]', word) == []]\n",
        "\n",
        "\n",
        "NWORDS_df = pd.read_csv('unigrams.cyr.lc', '\\t', names = [\"слово\", \"частота\", \"частота/милион\"])\n",
        "NWORDS_df = NWORDS_df.loc[~((NWORDS_df[\"слово\"].str.len() == 1) & (NWORDS_df[\"частота\"] < 4000))]\n",
        "NWORDS_df = NWORDS_df.loc[~((NWORDS_df[\"слово\"].str.len() == 2) & (NWORDS_df[\"частота\"] < 400))]\n",
        "NWORDS_2 = NWORDS_df[\"слово\"].tolist()\n",
        "NWORDS_2 = [word for word in NWORDS_2 if type(word) == str]\n",
        "NWORDS_2 = [word for word in NWORDS_2 if re.findall('[A-Za-z0-9]', word) == []]\n",
        "\n",
        "NWORDS = list(set(NWORDS_2 + NWORDS_1))\n",
        "\n",
        "root = aho_create_statemachine(NWORDS)\n",
        "\n",
        "print('\\n\\n///---------------- Автомат создан ----------------///\\n\\n')\n",
        "\n",
        "\n",
        "\n",
        "## СОЗДАНИЕ ОБУЧАЮЩЕЙ ВЫБОРКИ -----------------\n",
        "\n",
        "with codecs.open(\"annot.opcorpora.xml\", \"r\",encoding='utf-8', errors='ignore') as file:\n",
        "  list_of_sent = []\n",
        "  for pack in file.read().split(\"\\n\"):\n",
        "      try:\n",
        "          sent = pack.split('<source>')[1]\n",
        "          list_of_sent.append(re.sub('[-—,.–!?\\'\\\"…«»():]', '', sent).lower()[:-9])\n",
        "      except Exception:\n",
        "          \"wrong\"\n",
        "\n",
        "\n",
        "correct_ne_10 = tokenizing(list_of_sent_ne, 10)\n",
        "correct_ne_5 = tokenizing(list_of_sent_ne, 5)\n",
        "              \n",
        "correct_hard_10 = tokenizing(list_of_sent_hard, 10)\n",
        "correct_hard_5 = tokenizing(list_of_sent_hard, 5)\n",
        "\n",
        "\n",
        "\n",
        "list_of_sent_ne_unc = []\n",
        "\n",
        "for sent in list_of_sent_ne:\n",
        "  list_of_sent_ne_unc.append(re.sub(\"  \", \"\", (re.sub(\" не\", \" не \", sent))))\n",
        "\n",
        "all_uncorrect = tokenizing(list_of_sent_ne_unc, 10)\n",
        "uncorrect_ne_10 = [sent for sent in all_uncorrect if any(elem.startswith(\"не\") for elem in sent[1:len(sent)-1])]\n",
        "\n",
        "all_uncorrect = tokenizing(list_of_sent_ne_unc, 5)\n",
        "uncorrect_ne_5 = [sent for sent in all_uncorrect if any(elem.startswith(\"не\") for elem in sent[1:len(sent)-1])]\n",
        "\n",
        "\n",
        "\n",
        "list_of_sent_hurd_unc = []\n",
        "hard_pre = ['с', 'в', 'по', 'за', 'на', 'при', 'во', 'не']\n",
        "\n",
        "for sent in list_of_sent_hard:\n",
        "  for pre in hard_pre:\n",
        "    sent = (re.sub(\"  \", \"\", (re.sub(\" \" + pre, \" \" + pre + \" \", sent))))\n",
        "  list_of_sent_hurd_unc.append(sent)\n",
        "\n",
        "all_uncorrect = tokenizing(list_of_sent_hurd_unc, 10)\n",
        "uncorrect_hard_10 = [sent for sent in all_uncorrect if any(any(elem.startswith(pre) for pre in hard_pre) for elem in sent[1:len(sent)-1])]\n",
        "\n",
        "all_uncorrect = tokenizing(list_of_sent_hurd_unc, 5)\n",
        "uncorrect_hard_5 = [sent for sent in all_uncorrect if any(any(elem.startswith(pre) for pre in hard_pre) for elem in sent[1:len(sent)-1])]\n",
        "\n",
        "\n",
        "\n",
        "part_of_list_of_sent = list_of_sent[:12000]\n",
        "part_of_list_of_sent_wrong = []\n",
        "\n",
        "wr_list = []\n",
        "for sent in list_of_sent[-12000:]:\n",
        "  tmp_sent = sent.split(' ')\n",
        "  random.shuffle(tmp_sent)\n",
        "  part_of_list_of_sent_wrong.append(' '.join(tmp_sent))\n",
        "\n",
        "correct = tokenizing(part_of_list_of_sent, 9)\n",
        "uncorrect = tokenizing(part_of_list_of_sent_wrong, 9)\n",
        "\n",
        "all_tekenized_wrong_sent = uncorrect + uncorrect_ne_10 + uncorrect_ne_5 + uncorrect_hard_10 + uncorrect_hard_5\n",
        "all_tekenized_wrong_sent = np.array(all_tekenized_wrong_sent)\n",
        "\n",
        "all_tekenized_correct_sent = correct + correct_ne_10 + correct_ne_5 + correct_hard_10 + correct_hard_5\n",
        "all_tekenized_correct_sent = np.array(all_tekenized_correct_sent)\n",
        "# whole_list = part_of_list_of_sent + list_of_sent_ne + list_of_sent_hard\n",
        "# whole_list = np.array(whole_list)\n",
        "# all_tekenized_sent = tokenizing(whole_list, 12) + tokenizing(list_of_sent[-30000:], 2) \n",
        "\n",
        "y_train_1 = np.ones(len(all_tekenized_correct_sent))\n",
        "y_train_2 = np.zeros(len(all_tekenized_wrong_sent))\n",
        "\n",
        "X_train = np.hstack((all_tekenized_correct_sent, all_tekenized_wrong_sent))\n",
        "y_train = np.hstack((y_train_1, y_train_2))\n",
        "\n",
        "assert len(y_train) == len(X_train)\n",
        "\n",
        "\n",
        "\n",
        "## ПОДГОТВКА ОБУЧАЮЩЕЙ ВЫБОРКИ ДЛЯ ПОДАЧИ В МОДЕЛЬ ----------\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "tokens = tokenizer.texts_to_sequences(X_train)\n",
        "\n",
        "X_train_tok = np.array(tokens)\n",
        "X_train_tok = pad_sequences(X_train_tok, maxlen = 12)\n",
        "\n",
        "size_for = X_train_tok.reshape(1, 12*len(X_train_tok))\n",
        "\n",
        "size_of_dic = len(np.unique(size_for[0]))\n",
        "size_of_dic += 1\n",
        "\n",
        "# X_train_tok, X_test = test_data_sep(X_train_tok)\n",
        "# y_train, y_test = test_data_sep(y_train)\n",
        "\n",
        "assert len(X_train_tok) == len(y_train)\n",
        "\n",
        "\n",
        "\n",
        "## ОБУЧЕНИЕ МОДЕЛИ -----------\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(size_of_dic, 9, input_length = 12, dropout = 0.35))\n",
        "model.add(LSTM(100, return_sequences=True, dropout_W = 0.65, dropout_U = 0.60))\n",
        "model.add(LSTM(100, dropout=0.05))\n",
        "model.add(Dense(256, activation = 'relu'))\n",
        "model.add(Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss = 'binary_crossentropy',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "\n",
        "history = model.fit(X_train_tok, y_train, batch_size = 512, epochs = 15, validation_split=0.15)\n",
        "\n",
        "print('\\n\\n///---------------- Модель обучена ----------------///\\n\\n')\n",
        "\n",
        "plt.plot(history.history['accuracy'], \n",
        "         label='Доля верных ответов на обучающем наборе')\n",
        "plt.plot(history.history['val_accuracy'], \n",
        "         label='Доля верных ответов на проверочном наборе')\n",
        "plt.xlabel('Эпоха обучения')\n",
        "plt.ylabel('Доля верных ответов')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AAEjaYeIysI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbdofRfuSrRe",
        "colab_type": "text"
      },
      "source": [
        "# Исправление"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY9HN-KAyd3r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "28e95cee-0581-4268-f80b-0d8406579b38"
      },
      "source": [
        "text = open('/content/test_text.txt', 'r').read()\n",
        "\n",
        "\n",
        "print(text)\n",
        "\n",
        "text_ans = ''\n",
        "split_to_sentece = re.compile(r'[.|!|?|…]')\n",
        "split_to_part = (r'[\\W\\d]')\n",
        "flag_dot = 0\n",
        "if split_to_sentece.findall(text[-1]) == []:\n",
        "  flag_dot = 1\n",
        "  text += '.'\n",
        "\n",
        "sentence_splitter = split_to_sentece.findall(text)\n",
        "\n",
        "for sentence in split_to_sentece.split(text.lower())[:-1]:\n",
        "  counter = 0\n",
        "  not_letters = get_splitters(sentence)\n",
        "  sent_splited = get_split_sent(sentence)\n",
        "  # print(not_letters, sent_splited)\n",
        "\n",
        "  lists_of_words = [[elem for elem in sent.split(' ') if elem != ''] for sent in sent_splited]\n",
        "  words_in_sent = [elem for sent in sent_splited for elem in sent.split(' ') if elem != '']\n",
        "\n",
        "  final = ''\n",
        "  for i in range(len(sent_splited)):\n",
        "\n",
        "    flag = 1\n",
        "    jj = 0\n",
        "\n",
        "    while flag != 0:\n",
        "      flag = 0\n",
        "      \n",
        "      for j in range(jj, len(lists_of_words[i])):\n",
        "\n",
        "        count_of_words = len(lists_of_words[i])\n",
        "        # print(lists_of_words[i][j: j+5])\n",
        "        val = nearby_check_2(lists_of_words[i][j: j+5])\n",
        "\n",
        "        if (any(val[1:]) == True) or (val[0] == False):\n",
        "          flag = 1\n",
        "\n",
        "          if val[0] == False:\n",
        "            index = 0\n",
        "            # print(type(lists_of_words[i][j]))\n",
        "            # statement = ('qual' in m.analyze(lists_of_words[i][j])[0]['analysis'][0].keys())\n",
        "            statement = True\n",
        "          else:\n",
        "            index = max_val(val)\n",
        "\n",
        "          if statement or index != 0:\n",
        "\n",
        "\n",
        "            if i < len(sent_splited) - 1:\n",
        "              dop = lists_of_words[i+1][:(0 if j+7-count_of_words<0 else j+7-count_of_words)]\n",
        "\n",
        "            else:\n",
        "              dop = []\n",
        "\n",
        "            answer, n_val = correction(lists_of_words[i][(0 if j < 2 else j-2): j+7] + dop, j, count_of_words, index)\n",
        "            # print('answer = ' ,answer, n_val)\n",
        "\n",
        "            if answer != []:\n",
        "              if index == 0:\n",
        "                lists_of_words[i] = lists_of_words[i][:(0 if j < 2 else j-2)] + answer + lists_of_words[i][j+5-n_val:]\n",
        "              else:\n",
        "                lists_of_words[i] = lists_of_words[i][:(0 if j < 2 else j-2)] + answer + lists_of_words[i][j+index+1:]\n",
        "                jj = j + 2\n",
        "            else:\n",
        "              flag = 0\n",
        "          else:\n",
        "              flag = 0\n",
        "              jj = j + 2\n",
        "\n",
        "          break\n",
        "    \n",
        "    sign = ''\n",
        "\n",
        "    if len(not_letters) != 0:\n",
        "      if not_letters[0] not in ',;:':\n",
        "        sign = ' '\n",
        "      sign = sign + not_letters[0]\n",
        "      not_letters.remove(not_letters[0])\n",
        "\n",
        "    str_ = ' '.join(lists_of_words[i])\n",
        "    final = final + ' ' + str_ + sign\n",
        "\n",
        "\n",
        "  text_ans = text_ans + final[1:].capitalize() + sentence_splitter[0] + ' '\n",
        "  sentence_splitter.remove(sentence_splitter[0])\n",
        "\n",
        "text_ans = final_cor(text_ans, flag_dot)\n",
        "\n",
        "print(text_ans)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Это объясняется тем, что прои зошли большие из менения в жизни, в общественном и государственном устройстве, которыеоказаливлияниенаразвитие современного русского языка. Наиболее заметные проявления происходят в лексике: язык попо лн яет ся огр омным количеством заимствованных слов и оборотов, часто неоправданно. Однако существует и обратная сторона медали. Количество документов и другого контента резко растет — так, число элект ронных док ументов вотносите  льно крупной организации уже переваливает за миллионную отметку... С недавних пор вводится понятие электронного документооборота, так как объемы документов, производимых работниками умственного труда, стали измеряться не только папками, но и терабайтами. Отделымаркетингавыпускают пресс-релизы, отделы продаж составляют коммерческие предложения, юридические отделы штампуют договоры, отделы кадров формируют регламенты, высшее руководство издает приказы и трудится над новымист ратегическими презен  тациями. Документы — как бумажные, так и электронные — размножаются ссума сшедшей скоростью, что вызывает насущную необходимость электронного документооборота? Отсюда высокаяпотребность в правильнойиграмотной письме нной речи! Но самостоятельно человек просто невсостоянии отследить все ошибки и опечатки. Помощь в поиске и исправлении совершённых людьмиош и бок осуществляют так называемые спеллчекеры.\n",
            "Это объясняется тем, что произошли большие изменения в жизни, в общественном и государственном устройстве, которые оказали влияние на развитие современного русского языка. Наиболее заметные проявления происходят в лексике: язык пополняется огромным количеством заимствованных слов и оборотов, часто неоправданно. Однако существует и обратная сторона медали. Количество документов и другого контента резко растет — так, число электронных документов в относительно крупной организации уже переваливает за миллионную отметку... С недавних пор вводится понятие электронного документооборота, так как объемы документов, производимых работниками умственного труда, стали измеряться не только папками, но и терабайтами. Отделы маркетинга выпускают пресс-релизы, отделы продаж составляют коммерческие предложения, юридические отделы штампуют договоры, отделы кадров формируют регламенты, высшее руководство издает приказы и трудится над новыми стратегическими презентациями. Документы — как бумажные, таки электронные — размножаются с сумасшедшей скоростью, что вызывает насущную необходимость электронного документооборота? Отсюда высокая потребность в правильной и грамотной письменной речи! Но самостоятельно человек просто не в состоянии отследить все ошибки и опечатки. Помощь в поиске и исправлении совершённых людьми ошибок осуществляют так называемые спеллчекеры.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBhZr9xeydpz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}