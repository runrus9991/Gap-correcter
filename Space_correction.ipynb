{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGhQyGUhGx-c",
        "colab_type": "text"
      },
      "source": [
        "## Импорт библиотек"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfclaMo5Grp1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "acb04d3d-a53b-48a7-eb9a-712599474169"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding, Bidirectional, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from random import uniform\n",
        "from collections import defaultdict\n",
        "import requests\n",
        "import codecs\n",
        "import random\n",
        "import networkx as nx\n",
        "import math\n",
        "\n",
        "!pip install pymystem3==0.1.10\n",
        "import json\n",
        "from pymystem3 import Mystem\n",
        "\n",
        "m = Mystem()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting pymystem3==0.1.10\n",
            "  Downloading https://files.pythonhosted.org/packages/51/56/57e550b53587719e92330a79c7c0f555402d953b00700efae6d5ca53cdef/pymystem3-0.1.10-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pymystem3==0.1.10) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pymystem3==0.1.10) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pymystem3==0.1.10) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pymystem3==0.1.10) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pymystem3==0.1.10) (3.0.4)\n",
            "Installing collected packages: pymystem3\n",
            "  Found existing installation: pymystem3 0.2.0\n",
            "    Uninstalling pymystem3-0.2.0:\n",
            "      Successfully uninstalled pymystem3-0.2.0\n",
            "Successfully installed pymystem3-0.1.10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Installing mystem to /root/.local/bin/mystem from http://download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJnMa8aVG7mU",
        "colab_type": "text"
      },
      "source": [
        "## Обучение"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_MBu1tqG_ro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## АЛГОРИТМ АХО-КОРАСИКА ------------\n",
        "\n",
        "def graph_construction(list_of_words, sentence_len):\n",
        "    '''\n",
        "    Построение графа всех возможных разбиений.\n",
        "    list_of_words: множество всех возможных слов\n",
        "    sentence_len: длиннаа разбиения\n",
        "    '''\n",
        "    sentence_len += 2\n",
        "\n",
        "    G=nx.MultiDiGraph()\n",
        "    G.add_nodes_from(range(0, sentence_len))\n",
        "    G.number_of_nodes()\n",
        "    base_for_graph = []\n",
        "    \n",
        "    for i in range(0, len(list_of_words)) :\n",
        "        base_for_graph.append((list_of_words[i][1], str(i) + '_word'))\n",
        "        base_for_graph.append((str(i) + '_word', list_of_words[i][2]))\n",
        "\n",
        "    G.add_edges_from(base_for_graph)\n",
        "\n",
        "    return [[list_of_words[int(word.split('_')[0])][0] for word in word if type(word) != int] \\\n",
        "        for word in nx.all_simple_paths(G, 0, sentence_len - 2)]\n",
        "\n",
        "\n",
        "def edits1(word):\n",
        "    '''\n",
        "    Функция поиска всех возможных ближайших слов\n",
        "    (с наименьшим количеством перестановок)\n",
        "    модель Левенштейна\n",
        "    '''\n",
        "    n = len(word)\n",
        "    return list( [word[0:i]+word[i+1:] for i in range(n)] +                       # удаление символа\n",
        "                [word[0:i]+word[i+1]+word[i]+word[i+2:] for i in range(n-1)] +   # транспозиция соседних символов\n",
        "                [word[0:i]+c+word[i+1:] for i in range(n) for c in alphabet] +   # замена символа\n",
        "                [word[0:i]+c+word[i:] for i in range(n+1) for c in alphabet])   # вставка лишнего символа\n",
        "\n",
        "\n",
        "class AhoNode:\n",
        "# Класс для построения дерева\n",
        "    def __init__(self):\n",
        "        self.goto = {}\n",
        "        self.out = []\n",
        "        self.fail = None\n",
        "\n",
        "\n",
        "def aho_create_forest(patterns):\n",
        "    '''\n",
        "    Создать бор - дерево паттернов.\n",
        "    '''\n",
        "    root = AhoNode()\n",
        "\n",
        "    for path in patterns:\n",
        "        node = root\n",
        "\n",
        "        for symbol in path:\n",
        "            node = node.goto.setdefault(symbol, AhoNode())\n",
        "\n",
        "        node.out.append(path)\n",
        "    return root\n",
        "\n",
        "\n",
        "def aho_create_statemachine(patterns):\n",
        "    '''\n",
        "    Непосредственно содание автомата Ахо-Корасика.\n",
        "    '''\n",
        "    root = aho_create_forest(patterns)\n",
        "    queue = []\n",
        "\n",
        "    for node in root.goto.values():\n",
        "        queue.append(node)\n",
        "        node.fail = root\n",
        "        \n",
        "    while len(queue) > 0:\n",
        "        rnode = queue.pop(0)\n",
        "\n",
        "        for key, unode in rnode.goto.items():\n",
        "            queue.append(unode)\n",
        "            fnode = rnode.fail\n",
        "            while fnode is not None and key not in fnode.goto:\n",
        "                fnode = fnode.fail\n",
        "            unode.fail = fnode.goto[key] if fnode else root\n",
        "            unode.out += unode.fail.out\n",
        "\n",
        "    return root\n",
        "\n",
        "\n",
        "def aho_find_all(s, root):\n",
        "    '''\n",
        "    Находит все возможные подстроки из набора узлов в строке.\n",
        "    '''\n",
        "    founded_words = []\n",
        "    node = root\n",
        "\n",
        "    for i in range(len(s)):\n",
        "        while node is not None and s[i] not in node.goto:\n",
        "            node = node.fail\n",
        "        if node is None:\n",
        "            node = root\n",
        "            continue\n",
        "        node = node.goto[s[i]]\n",
        "\n",
        "        for pattern in node.out:\n",
        "             founded_words.append((pattern, i - len(pattern) + 1, i + 1))\n",
        "    return founded_words\n",
        "\n",
        "\n",
        "def my_algorithm_start(string_of_letters, root):\n",
        "    '''\n",
        "    Запуск алгоритма.\n",
        "    '''\n",
        "    return merge_all_lists([aho_find_all(new_string, root) for new_string in [string_of_letters]])\n",
        "\n",
        "\n",
        "def merge_all_lists(lstlst):\n",
        "    all_lists=[]\n",
        "    for lst in lstlst:\n",
        "        all_lists.extend(lst)\n",
        "    return all_lists\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## ФУНКЦИИ ДЛЯ ПРЕДОБРАБОТКИ СТРОК ---------------\n",
        "\n",
        "def test_data_sep(data):\n",
        "  return data[1500 : -1500], np.hstack((data[:1500], data[-1500:]))\n",
        "\n",
        "\n",
        "def is_in(list_, vall):\n",
        "    return vall in list(map(lambda x:x[0], list_))\n",
        "\n",
        "\n",
        "def max_val(mass):\n",
        "  '''\n",
        "  Возвращает индекс последнего вхождения максимального\n",
        "  элемента в список.\n",
        "  '''\n",
        "  return next(i for i in range(len(mass)-1, -1, -1) if mass[i] == max(mass))\n",
        "\n",
        "\n",
        "def nearby_check_2(words):\n",
        "  '''\n",
        "  Проверка существуют ли слова получаемые объединением с \n",
        "  соседними словами и сущесттвует ли такое слово само.\n",
        "  '''\n",
        "  len_ = len(words) + 1\n",
        "\n",
        "  if words[0] != '':\n",
        "    return [is_in(my_algorithm_start(''.join(words[:i]), root), ''.join(words[:i])) for i in range(1, len_)] + \\\n",
        "      [False for i in range(len_, 6)]\n",
        "  else: \n",
        "    return [False]\n",
        "\n",
        "\n",
        "def min_list(lists):\n",
        "  min_l = []\n",
        "  min_len =  math.inf\n",
        "\n",
        "  for list_ in lists:\n",
        "    if len(list_) < min_len:\n",
        "      min_l = []\n",
        "      min_len = len(list_)\n",
        "      min_l.append(list_)\n",
        "    elif len(list_) == min_len:\n",
        "      min_l.append(list_)\n",
        "\n",
        "  if len(min_l) == 1:\n",
        "    return min_l[0]\n",
        "\n",
        "  else:\n",
        "    return answer_func([], min_l, [])\n",
        "\n",
        "\n",
        "def min_list_len(lists):\n",
        "  '''\n",
        "  Длинна минимального списка.\n",
        "  '''\n",
        "  min_len =  math.inf\n",
        "\n",
        "  for list_ in lists:\n",
        "    if len(list_) < min_len:\n",
        "      min_len = len(list_)\n",
        "\n",
        "  return min_len\n",
        "\n",
        "\n",
        "def answer_func(left_cont, list_of_options, right_cont):\n",
        "  '''\n",
        "  left_cont: левый контекст\n",
        "  list_of_options: все возможные разбиения\n",
        "  right_cont: правый контекст\n",
        "  '''\n",
        "  probabilities = []\n",
        "  \n",
        "  for sent in list_of_options:\n",
        "    count = 0\n",
        "    # prob = -0.5 * (len(sent) - 1)\n",
        "    prob = (math.exp((len(sent) - min_list_len(list_of_options))*0.55) - 1) * -0.1\n",
        "    sent_ = left_cont + sent + right_cont\n",
        "\n",
        "    while len(sent_) > 0:\n",
        "      paded_sent, sent_ = sent_[:12], sent_[12:]\n",
        "      count += 1\n",
        "      prob += model.predict(pad_sequences(np.array(tokenizer.texts_to_sequences([' '.join(paded_sent)])), maxlen = 12))\n",
        "\n",
        "    probabilities.append(prob[0][0]/count)\n",
        "\n",
        "  # print(list_of_options, '\\n', probabilities)\n",
        "  return list_of_options[max_val(probabilities)]\n",
        "\n",
        "\n",
        "def correction(part_of_sent, j, count_of_words, index):\n",
        "  '''\n",
        "  Вызов алгоритма исправления.\n",
        "  text_ans: список слов с ошибками\n",
        "  j: номер слова в котором обнаружена ошибка\n",
        "  count_of_words: количество слов в данном разбиении\n",
        "  index: тип ошибки\n",
        "  '''\n",
        "  max_len = count_of_words - j + 2\n",
        "  list_of_fragmentation = []\n",
        "  if max_len > 7:\n",
        "    n = 7 \n",
        "  else:\n",
        "    n = max_len - 1\n",
        "  # print('j = ', j, 'count_of_words = ', max_len)\n",
        "\n",
        "  # print(part_of_sent)\n",
        "\n",
        "  if index == 0:\n",
        "    while n > 1 and list_of_fragmentation == []:\n",
        "      # print('n = ', n)\n",
        "\n",
        "      if j == 0:\n",
        "        merged_s = ''.join(part_of_sent[0: (max_len - 2 if n - 1 > max_len else n - 1)])\n",
        "        left_cont = []\n",
        "        right_cont = part_of_sent[(max_len - 2 if n - 1 > max_len else n - 1):]\n",
        "\n",
        "      elif j == 1:\n",
        "        merged_s = ''.join(part_of_sent[0:(max_len - 1 if n > max_len else n)])\n",
        "        left_cont = []\n",
        "        right_cont = part_of_sent[(max_len - 1 if n > max_len else n):]\n",
        "\n",
        "      else:\n",
        "        merged_s = ''.join(part_of_sent[1:(max_len if n + 1 > max_len else n + 1)])\n",
        "        left_cont = part_of_sent[:1]\n",
        "        right_cont = part_of_sent[(max_len if n + 1 > max_len else n + 1):]\n",
        "        # print(merged_s)\n",
        "\n",
        "      list_of_sent = my_algorithm_start(merged_s, root)\n",
        "\n",
        "      list_of_fragmentation = graph_construction(list_of_sent, len(merged_s))\n",
        "      n -= 1\n",
        "\n",
        "  else:\n",
        "\n",
        "    if j == 0:\n",
        "      merged_s = ''.join(part_of_sent[0:(max_len if index + 1 > max_len else index + 1)])\n",
        "      left_cont = []\n",
        "      right_cont = part_of_sent[(max_len if index + 1 > max_len else index + 1):]\n",
        "\n",
        "    elif j == 1:\n",
        "      merged_s = ''.join(part_of_sent[1:(max_len if index + 2 > max_len else index + 2)])\n",
        "      left_cont = part_of_sent[:1]\n",
        "      right_cont = part_of_sent[(max_len if index + 2 > max_len else index + 2):]\n",
        "\n",
        "    else:\n",
        "      merged_s = ''.join(part_of_sent[2:(max_len if index + 3 > max_len else index + 3)])\n",
        "      left_cont = part_of_sent[:2]\n",
        "      right_cont = part_of_sent[(max_len if index + 3 > max_len else index + 3):]\n",
        "\n",
        "    list_of_sent = my_algorithm_start(merged_s, root)\n",
        "    list_of_fragmentation = graph_construction(list_of_sent, len(merged_s))\n",
        "\n",
        "  if list_of_fragmentation == []:\n",
        "    return [], 0\n",
        "\n",
        "  if index == 0:\n",
        "    return left_cont + min_list(list_of_fragmentation), 5 - n\n",
        "  \n",
        "  else:\n",
        "    return left_cont + answer_func(left_cont, list_of_fragmentation, right_cont), 0\n",
        "\n",
        "\n",
        "def final_cor(text_ans, flag_dot):\n",
        "  '''\n",
        "  Финальная обработка предложения.\n",
        "  text_ans: исправленный текст\n",
        "  flag_dot: индикатор нахождения заключащего \n",
        "  \"точки\" в конце последнего предложения\n",
        "  '''\n",
        "  text_ans = re.sub(\" - \", \"-\", text_ans)\n",
        "  text_ans = re.sub(\"  \", \" \", text_ans)\n",
        "  text_ans = re.sub(\"\\. \\. \\.\", \"...\", text_ans)\n",
        "  if flag_dot == 1:\n",
        "    text_ans = text_ans[:-2]\n",
        "  return text_ans\n",
        "\n",
        "\n",
        "def get_splitters(sentence):\n",
        "  '''\n",
        "  Выделяет все небуквенные символы в предложениии.\n",
        "  sentence: предложение (строка сиволов)\n",
        "  '''\n",
        "  not_letters = ' '.join(re.findall(split_to_part, sentence)).split(' ')\n",
        "  return [elem for elem in not_letters if elem != '']\n",
        "\n",
        "\n",
        "def get_split_sent(sentence):\n",
        "  '''\n",
        "  Преобразует предложение в список фраз отделёных\n",
        "  друг от друга разделителями.\n",
        "  sentence: предложение (строка сиволов)\n",
        "  '''\n",
        "  clear_sent = re.sub('><', '', re.sub('[^а-яё ]', '<>', sentence))\n",
        "  return [elem for elem in clear_sent.split('<>') if elem != '']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## РАССЧЁТ ТОЧНОСТИ --------------\n",
        "\n",
        "def mistake_generator(text, const_err = 1):\n",
        "  '''\n",
        "  Генерируются ошибки в поданом на вход тексте.\n",
        "  text: входной текст, в котором будут сгенерированы ошибки\n",
        "  const_err: примерное количество совершённых ошибок на \n",
        "  100 символов\n",
        "  '''\n",
        "  answ_text = ''\n",
        "  text += '.'\n",
        "  mistakes_count = 0\n",
        "  split_to_sentece = re.compile(r'[.|!|?|…|\\n]')\n",
        "  sentence_splitter_m = split_to_sentece.findall(text)\n",
        "\n",
        "  for sent in split_to_sentece.split(text)[:-1]:\n",
        "      tmp_sent = ''\n",
        "      for letter in sent:\n",
        "          if random.random() * 100 > 100 - const_err:\n",
        "              mistakes_count += 1\n",
        "              \n",
        "              if letter != ' ':\n",
        "                  tmp_sent += ' '\n",
        "\n",
        "              else:\n",
        "                  continue\n",
        "                  \n",
        "          tmp_sent += letter \n",
        "\n",
        "      try:\n",
        "          answ_text += tmp_sent + sentence_splitter_m[0]\n",
        "          sentence_splitter.remove(sentence_splitter_m[0])\n",
        "        \n",
        "      except Exception:\n",
        "          'no end phrase sign'\n",
        "\n",
        "  return answ_text[:-1], mistakes_count\n",
        "\n",
        "\n",
        "\n",
        "def evaluetion(text, text_with_errors, corected_text):\n",
        "  '''\n",
        "  Рассчитываются знвчения Precision, Recall и F-меры.\n",
        "  text: эталонный текст без ошибок\n",
        "  text_with_errors: текст с внесёнными ошибками\n",
        "  corected_text: текст с исправленными ошибками\n",
        "  '''\n",
        "  jl_text = ''.join(re.findall('[\\w ]', text.lower()))\n",
        "  jl_text_with_errors = re.sub('  ', ' ', ' ' + ''.join(re.findall('[\\w ]', text_with_errors.lower())))\n",
        "  jl_corected_text = re.sub('  ', ' ', ' ' + ''.join(re.findall('[\\w ]', corected_text.lower())))\n",
        "\n",
        "\n",
        "  tp = 0 # количество верно исправленных ошибок\n",
        "  fp = 0 # количество 'исправленных' изначально правильных слов\n",
        "  fn = 0 # количество пропущенных или исправленных неверно ошибок\n",
        "\n",
        "\n",
        "  for word in jl_text.split(' '):\n",
        "      we_counter = 0\n",
        "      c_counter = 0\n",
        "      we_word , c_word = '', ''\n",
        "\n",
        "      i = 0\n",
        "      while we_counter < len(word):\n",
        "          if jl_text_with_errors[i] != ' ':\n",
        "              we_counter += 1\n",
        "          we_word += jl_text_with_errors[i]\n",
        "          i += 1\n",
        "\n",
        "      jl_text_with_errors = jl_text_with_errors[i:]\n",
        "\n",
        "      j = 0\n",
        "      while c_counter < len(word):\n",
        "          if jl_corected_text[j] != ' ':\n",
        "              c_counter += 1\n",
        "          c_word += jl_corected_text[j]\n",
        "          j += 1\n",
        "\n",
        "      jl_corected_text = jl_corected_text[j:]\n",
        "\n",
        "      if c_word[1:] == word and we_word != c_word:\n",
        "          tp += 1\n",
        "\n",
        "      elif c_word[1:] != word:\n",
        "          if we_word[1:] == word:\n",
        "              fp += 1\n",
        "              \n",
        "          else:\n",
        "              fn += 1\n",
        "\n",
        "  try:\n",
        "    precision = tp / (tp + fp)\n",
        "  except ZeroDivisionError:\n",
        "    precision = 1\n",
        "  try: \n",
        "    recall = tp / (tp + fn)\n",
        "  except ZeroDivisionError:\n",
        "    recall = 1\n",
        "\n",
        "  F_measure = 2 * precision * recall / (precision + recall)\n",
        "  \n",
        "  return precision, recall, F_measure, fp + fn\n",
        "\n",
        "\n",
        "\n",
        "## ЗАГРУЗКА ДАННЫХ --------------\n",
        "\n",
        "def load_data():\n",
        "  '''\n",
        "  Загрузка данных для обучения.\n",
        "  '''\n",
        "  # НЕ С НАРЕЧИЯМИ\n",
        "  word_list_1 = ['невысоко', 'немало', 'немного', 'неплохо', 'несильно', 'недорого', 'неслабо', 'нехорошо', 'непросто', 'нелегко', 'негде', 'незачем', 'некуда', \n",
        "                'неоткуда', 'невдалеке', 'невмоготу', 'невмочь', 'невпопад', 'невтерпеж', 'недаром', 'невесть', 'не высоко а', 'не мало а', 'не много а', \n",
        "                'не плохо а', 'не сильно а', 'не дорого а', 'не слабо а', 'не хорошо а', 'не просто а', 'не легко а', 'далеко не', 'вовсе не', 'отнюдь не', \n",
        "                'ничуть не', 'нисколько не', 'нисколько не', 'не вполне', 'не здесь', 'не очень', 'не полностью', 'не совсем','не там', 'не туда', 'несмотря на', \n",
        "                'невзирая на', 'не смотря', 'не взирая']\n",
        "\n",
        "\n",
        "  # ОСТАЛЬНЫЕ СЛОЖНЫЕ СЛУЧАИ СЛИТНО-РАЗДЕЛЬНОГО НАПИСАНИЯ\n",
        "  word_list_2 = ['в течение', 'в деле', 'в продолжение', 'в силу', 'в виде', 'в области', 'в связи', 'в смысле', 'по мере', 'по причине', 'в меру', 'за исключением',\n",
        "                'в отличие', 'по поводу', 'в отношении', 'иметь в виду', 'ввиду', 'навстречу', 'вслед', 'внутри', 'вроде', 'вместо', 'вследствие', 'наподобие', \n",
        "                'насчёт', 'сверх', 'впросак', 'наобум', 'насмарку', 'взаперти', 'всмятку', 'испокон', 'взатяжку', 'вброд', 'вслух', 'при том', 'что бы', 'притом',\n",
        "                'вовремя', 'вволю', 'напоказ', 'вверх', 'снизу', 'вперёд', 'вглубь', 'ввек', 'наверх', 'вплотную', 'наудалую', 'зачастую', 'наглухо', 'сгоряча', \n",
        "                'набело', 'заживо', 'вдвое', 'натрое', 'вовсю', 'почему', 'поэтому', 'затем', 'доныне', 'навсегда', 'задаром', 'вполголоса', 'вполсилы', 'во время', \n",
        "                'по двое', 'по трое', 'по одному', 'по этому', 'по тому', 'за тем', 'поэтому', 'потому', 'затем ', 'во всю ширь', 'без оглядки', 'до упора', \n",
        "                'с виду', 'под стать', 'чтобы', 'от того', 'оттого', 'при чём']\n",
        "\n",
        "\n",
        "  # СЛОВАРЬ ВСЕХ ВОЗМОЖНЫХ СЛОВ (ruscorpora)\n",
        "  !wget http://ruscorpora.ru/ngrams/1grams-3.zip\n",
        "  !unzip 1grams-3.zip\n",
        "\n",
        "\n",
        "  # СЛОВАРЬ ВСЕХ ВОЗМОЖНЫХ СЛОВ (opencorpora)\n",
        "  !wget http://opencorpora.org/files/export/ngrams/unigrams.cyr.lc.zip\n",
        "  !unzip unigrams.cyr.lc.zip\n",
        "\n",
        "\n",
        "  # КОРПУС РАЗМЕЧЕННЫХ ПРЕДЛОЖЕНИЙ (opencorpora)\n",
        "  !wget http://opencorpora.org/files/export/annot/annot.opcorpora.xml.zip\n",
        "  !unzip annot.opcorpora.xml.zip\n",
        "\n",
        "  return word_list_1, word_list_2\n",
        "\n",
        "\n",
        "def download_sentances(list_of_words_in):\n",
        "  '''\n",
        "  Выгрузка всех предложений содержащих указанные в \n",
        "  аргументе функции слова с сайта https://kartaslov.ru.\n",
        "  list_of_words_in: список слов\n",
        "  '''\n",
        "  lst_in = []\n",
        "\n",
        "  for word in list_of_words_in:\n",
        "    download_link = 'https://kartaslov.ru/предложения-со-словом/' + word\n",
        "\n",
        "    parsed_sait = requests.get(download_link)\n",
        "    splited_data = parsed_sait.text.split('class=\"v2-sentence-box\">\\r\\n            ')[1:]\n",
        "    for sent in splited_data:\n",
        "      \n",
        "      lst_in.append(re.sub('[-—,.–!?\\'\\\"…«»</b>&laquo&raquo;():]', '', sent.split(\"        \")[0]).lower())\n",
        "    \n",
        "  return lst_in\n",
        "\n",
        "\n",
        "def tokenizing(whole_list, nn):\n",
        "  '''\n",
        "  Разбивает предложения на списки необходимой длинны \n",
        "  whole_list: список предложений\n",
        "  nn: максимальная длинна разбиения\n",
        "  '''\n",
        "  all_tekenized_sent = []\n",
        "  # tokenizer\n",
        "\n",
        "  for sentence in whole_list:\n",
        "\n",
        "      tmp_sentence = re.sub('  ', ' ', ''.join(re.findall('[а-яА-Я ]', sentence)))\n",
        "\n",
        "      tokenized_sent = []\n",
        "      \n",
        "      word_counter = 5\n",
        "      splited_sentence = tmp_sentence.split(\" \")\n",
        "      for word in splited_sentence[3:-1]:\n",
        "\n",
        "          sent = []\n",
        "          tmp_word_counter = word_counter - 5\n",
        "\n",
        "          while tmp_word_counter - word_counter < nn - 5:\n",
        "\n",
        "              if (tmp_word_counter < 0 or tmp_word_counter > len(splited_sentence) - 1) == False:\n",
        "                  sent.append(splited_sentence[tmp_word_counter])\n",
        "\n",
        "              tmp_word_counter += 1\n",
        "\n",
        "          all_tekenized_sent.append(sent)\n",
        "          word_counter += 1\n",
        "\n",
        "  return all_tekenized_sent\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## ПОДГОТОВКА ДАННЫХ ДЛЯ МОДЕЛИ -----------\n",
        "\n",
        "def trainig_sample_creation(size, hard_part = True):\n",
        "  '''\n",
        "  Создание тренировочной выборки\n",
        "  size: колличество предложений используемых для обучения\n",
        "  hard_part: (type boolean) индикатор использования предложений \n",
        "  загруженных с сайта https://kartaslov.ru/\n",
        "  '''\n",
        "  part_of_list_of_sent = list_of_sent[:size]\n",
        "  part_of_list_of_sent_wrong = []\n",
        "\n",
        "  wr_list = []\n",
        "  for sent in list_of_sent[-size:]:\n",
        "    tmp_sent = sent.split(' ')\n",
        "    random.shuffle(tmp_sent)\n",
        "    part_of_list_of_sent_wrong.append(' '.join(tmp_sent))\n",
        "\n",
        "  correct = tokenizing(part_of_list_of_sent, 12) #+ tokenizing(part_of_list_of_sent[50000:], 6)\n",
        "  uncorrect = tokenizing(part_of_list_of_sent_wrong, 12) #+ tokenizing(part_of_list_of_sent_wrong[50000:], 6)\n",
        "\n",
        "  all_tekenized_wrong_sent = uncorrect \n",
        "  if hard_part == True:\n",
        "    all_tekenized_wrong_sent += uncorrect_ne_10 + uncorrect_ne_5 + uncorrect_hard_10 + uncorrect_hard_5\n",
        "  all_tekenized_wrong_sent = np.array(all_tekenized_wrong_sent)\n",
        "\n",
        "  all_tekenized_correct_sent = correct \n",
        "  if hard_part == True:\n",
        "    all_tekenized_correct_sent += correct_ne_10 + correct_ne_5 + correct_hard_10 + correct_hard_5\n",
        "  all_tekenized_correct_sent = np.array(all_tekenized_correct_sent)\n",
        "  # whole_list = part_of_list_of_sent + list_of_sent_ne + list_of_sent_hard\n",
        "  # whole_list = np.array(whole_list)\n",
        "  # all_tekenized_sent = tokenizing(whole_list, 12) + tokenizing(list_of_sent[-30000:], 2) \n",
        "\n",
        "  y_train_1 = np.ones(len(all_tekenized_correct_sent))\n",
        "  y_train_2 = np.zeros(len(all_tekenized_wrong_sent))\n",
        "\n",
        "  X_train = np.hstack((all_tekenized_correct_sent, all_tekenized_wrong_sent))\n",
        "  y_train = np.hstack((y_train_1, y_train_2))\n",
        "\n",
        "  assert len(y_train) == len(X_train)\n",
        "\n",
        "  ## ТОКЕНИЗАЯ ОБУЧАЮЩЕЙ ВЫБОРКИ ДЛЯ ПОДАЧИ В МОДЕЛЬ ----------\n",
        "\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "  tokens = tokenizer.texts_to_sequences(X_train)\n",
        "\n",
        "  X_train_tok = np.array(tokens)\n",
        "  X_train_tok = pad_sequences(X_train_tok, maxlen = 12)\n",
        "\n",
        "  size_for = X_train_tok.reshape(1, 12*len(X_train_tok))\n",
        "\n",
        "  size_of_dic = len(np.unique(size_for[0]))\n",
        "  size_of_dic += 1\n",
        "\n",
        "  # X_train_tok, X_test = test_data_sep(X_train_tok)\n",
        "  # y_train, y_test = test_data_sep(y_train)\n",
        "\n",
        "  assert len(X_train_tok) == len(y_train)\n",
        "  \n",
        "  return X_train_tok, y_train, size_of_dic\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## НАЧАЛЬНЫЕ ЗНАЧЕНИЯ ---------------\n",
        "\n",
        "word_list_1, word_list_2 = load_data()\n",
        "\n",
        "list_of_sent_ne = download_sentances(word_list_1)\n",
        "list_of_sent_hard = download_sentances(word_list_2)\n",
        "\n",
        "split_regex = re.compile(r'[.|!|?|…]')\n",
        "list_of_wrong_sent = []\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "# wrong_text = open(\"/content/text2.txt\", \"r\",encoding='utf-8', errors='ignore').read()\n",
        "# wrong_text_list = wrong_text.split(' ')\n",
        "# random.shuffle(wrong_text_list) \n",
        "# wrong_tok = re.sub('  ', ' ', ' '.join(wrong_text_list))\n",
        "\n",
        "# for wrong_sentence in split_regex.split(wrong_tok):\n",
        "#   list_of_wrong_sent.append(re.sub('[-—,.–!?\\'\\\"…«»():]', '', wrong_sentence).lower())\n",
        "\n",
        "\n",
        "\n",
        "## СОЗДАНИЕ АВТОМАТА -------------\n",
        "\n",
        "NWORDS_df = pd.read_csv('1grams-3.txt', '\\t', names = [\"частота\", \"слово\"])\n",
        "\n",
        "NWORDS_df = NWORDS_df.loc[NWORDS_df[\"слово\"].str.len() > 6]\n",
        "NWORDS_1 = NWORDS_df[\"слово\"].tolist()\n",
        "NWORDS_1 = [word for word in NWORDS_1 if type(word) == str]\n",
        "NWORDS_1 = [word for word in NWORDS_1 if re.findall('[A-Za-z0-9]', word) == []]\n",
        "\n",
        "\n",
        "NWORDS_df = pd.read_csv('unigrams.cyr.lc', '\\t', names = [\"слово\", \"частота\", \"частота/милион\"])\n",
        "NWORDS_df = NWORDS_df.loc[~((NWORDS_df[\"слово\"].str.len() == 1) & (NWORDS_df[\"частота\"] < 4000))]\n",
        "NWORDS_df = NWORDS_df.loc[~((NWORDS_df[\"слово\"].str.len() == 2) & (NWORDS_df[\"частота\"] < 400))]\n",
        "NWORDS_2 = NWORDS_df[\"слово\"].tolist()\n",
        "NWORDS_2 = [word for word in NWORDS_2 if type(word) == str]\n",
        "NWORDS_2 = [word for word in NWORDS_2 if re.findall('[A-Za-z0-9]', word) == []]\n",
        "\n",
        "NWORDS = list(set(NWORDS_2 + NWORDS_1))\n",
        "\n",
        "root = aho_create_statemachine(NWORDS)\n",
        "\n",
        "print('\\n\\n///---------------- Автомат создан ----------------///\\n\\n')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## СОЗДАНИЕ ОБУЧАЮЩЕЙ ВЫБОРКИ -----------------\n",
        "\n",
        "with codecs.open(\"annot.opcorpora.xml\", \"r\",encoding='utf-8', errors='ignore') as file:\n",
        "  list_of_sent = []\n",
        "  for pack in file.read().split(\"\\n\"):\n",
        "      try:\n",
        "          sent = pack.split('<source>')[1]\n",
        "          list_of_sent.append(re.sub('[-—,.–!?\\'\\\"…«»():]', '', sent).lower()[:-9])\n",
        "      except Exception:\n",
        "          \"wrong\"\n",
        "\n",
        "\n",
        "correct_ne_10 = tokenizing(list_of_sent_ne, 10)\n",
        "correct_ne_5 = tokenizing(list_of_sent_ne, 5)\n",
        "              \n",
        "correct_hard_10 = tokenizing(list_of_sent_hard, 10)\n",
        "correct_hard_5 = tokenizing(list_of_sent_hard, 5)\n",
        "\n",
        "\n",
        "list_of_sent_ne_unc = []\n",
        "\n",
        "for sent in list_of_sent_ne:\n",
        "  list_of_sent_ne_unc.append(re.sub(\"  \", \"\", (re.sub(\" не\", \" не \", sent))))\n",
        "\n",
        "all_uncorrect = tokenizing(list_of_sent_ne_unc, 10)\n",
        "uncorrect_ne_10 = [sent for sent in all_uncorrect if any(elem.startswith(\"не\") for elem in sent[1:len(sent)-1])]\n",
        "\n",
        "all_uncorrect = tokenizing(list_of_sent_ne_unc, 5)\n",
        "uncorrect_ne_5 = [sent for sent in all_uncorrect if any(elem.startswith(\"не\") for elem in sent[1:len(sent)-1])]\n",
        "\n",
        "\n",
        "list_of_sent_hurd_unc = []\n",
        "hard_pre = ['с', 'в', 'по', 'за', 'на', 'при', 'во', 'не']\n",
        "\n",
        "for sent in list_of_sent_hard:\n",
        "  for pre in hard_pre:\n",
        "    sent = (re.sub(\"  \", \"\", (re.sub(\" \" + pre, \" \" + pre + \" \", sent))))\n",
        "  list_of_sent_hurd_unc.append(sent)\n",
        "\n",
        "all_uncorrect = tokenizing(list_of_sent_hurd_unc, 10)\n",
        "uncorrect_hard_10 = [sent for sent in all_uncorrect if any(any(elem.startswith(pre) for pre in hard_pre) for elem in sent[1:len(sent)-1])]\n",
        "\n",
        "all_uncorrect = tokenizing(list_of_sent_hurd_unc, 5)\n",
        "uncorrect_hard_5 = [sent for sent in all_uncorrect if any(any(elem.startswith(pre) for pre in hard_pre) for elem in sent[1:len(sent)-1])]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## ОБУЧЕНИЕ МОДЕЛИ -----------\n",
        "\n",
        "X_train_tok, y_train, size_of_dic = trainig_sample_creation(100000)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(size_of_dic, 9, input_length = 12, dropout = 0.35))\n",
        "model.add(LSTM(100, return_sequences=True, dropout_W = 0.2, dropout_U = 0.25))\n",
        "model.add(LSTM(100, dropout=0.15))\n",
        "model.add(Dense(128, activation = 'relu'))\n",
        "model.add(Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss = 'binary_crossentropy',\n",
        "              metrics = ['accuracy']\n",
        "             )\n",
        "\n",
        "\n",
        "history = model.fit(X_train_tok, y_train, batch_size = 64, epochs = 10, validation_split=0.15)\n",
        "\n",
        "print('\\n\\n///---------------- Модель обучена ----------------///\\n\\n')\n",
        "\n",
        "plt.plot(history.history['accuracy'], \n",
        "         label='Доля верных ответов на обучающем наборе')\n",
        "plt.plot(history.history['val_accuracy'], \n",
        "         label='Доля верных ответов на проверочном наборе')\n",
        "plt.xlabel('Эпоха обучения')\n",
        "plt.ylabel('Доля верных ответов')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYW8pxINHSEy",
        "colab_type": "text"
      },
      "source": [
        "## Исправление"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRdpOyknHVsS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "6c87b4cb-8497-4cf4-f691-15313d6a57ee"
      },
      "source": [
        "# text = open('/content/test_text.txt', 'r').read()\n",
        "standard = open('/content/standard.txt', 'r').read()\n",
        "\n",
        "text, mist = mistake_generator(standard, 3)\n",
        "\n",
        "\n",
        "\n",
        "# print('\\n\\n///---------------- ПРИМЕР ----------------///\\n\\nТекст с ошибками: \\n', text)\n",
        "\n",
        "text_ans = ''\n",
        "split_to_sentece = re.compile(r'[.|!|?|…]')\n",
        "split_to_part = (r'[\\W\\d]')\n",
        "flag_dot = 0\n",
        "if split_to_sentece.findall(text[-1]) == []:\n",
        "  flag_dot = 1\n",
        "  text += '.'\n",
        "\n",
        "sentence_splitter = split_to_sentece.findall(text)\n",
        "\n",
        "for sentence in split_to_sentece.split(text.lower())[:-1]:\n",
        "  counter = 0\n",
        "  not_letters = get_splitters(sentence)\n",
        "  sent_splited = get_split_sent(sentence)\n",
        "\n",
        "  lists_of_words = [[elem for elem in sent.split(' ') if elem != ''] for sent in sent_splited]\n",
        "  words_in_sent = [elem for sent in sent_splited for elem in sent.split(' ') if elem != '']\n",
        "\n",
        "  final = ''\n",
        "  for i in range(len(sent_splited)):\n",
        "\n",
        "    flag = 1\n",
        "    jj = 0\n",
        "\n",
        "    while flag != 0:\n",
        "      flag = 0\n",
        "      \n",
        "      for j in range(jj, len(lists_of_words[i])):\n",
        "\n",
        "        count_of_words = len(lists_of_words[i])\n",
        "        val = nearby_check_2(lists_of_words[i][j: j+5])\n",
        "\n",
        "        if (any(val[1:]) == True) or (val[0] == False):\n",
        "          flag = 1\n",
        "\n",
        "          if val[0] == False:\n",
        "            index = 0\n",
        "            # statement = ('qual' in m.analyze(lists_of_words[i][j])[0]['analysis'][0].keys())\n",
        "            statement = True\n",
        "          else:\n",
        "            statement = True\n",
        "            index = max_val(val)\n",
        "\n",
        "          if statement or index != 0:\n",
        "\n",
        "\n",
        "            if i < len(sent_splited) - 1:\n",
        "              dop = lists_of_words[i+1][:(0 if j+7-count_of_words<0 else j+7-count_of_words)]\n",
        "\n",
        "            else:\n",
        "              dop = []\n",
        "\n",
        "            answer, n_val = correction(lists_of_words[i][(0 if j < 2 else j-2): j+7] + dop, j, count_of_words, index)\n",
        "\n",
        "            if answer != []:\n",
        "              if index == 0:\n",
        "                lists_of_words[i] = lists_of_words[i][:(0 if j < 2 else j-2)] + answer + lists_of_words[i][j+5-n_val:]\n",
        "              else:\n",
        "                lists_of_words[i] = lists_of_words[i][:(0 if j < 2 else j-2)] + answer + lists_of_words[i][j+index+1:]\n",
        "                jj = j + 2\n",
        "            else:\n",
        "              flag = 0\n",
        "          else:\n",
        "              flag = 0\n",
        "              jj = j + 2\n",
        "\n",
        "          break\n",
        "    \n",
        "    sign = ''\n",
        "\n",
        "    if len(not_letters) != 0:\n",
        "      if not_letters[0] not in ',;:':\n",
        "        sign = ' '\n",
        "      sign = sign + not_letters[0]\n",
        "      not_letters.remove(not_letters[0])\n",
        "\n",
        "    str_ = ' '.join(lists_of_words[i])\n",
        "    final = final + ' ' + str_ + sign\n",
        "\n",
        "\n",
        "  text_ans = text_ans + final[1:].capitalize() + sentence_splitter[0] + ' '\n",
        "  sentence_splitter.remove(sentence_splitter[0])\n",
        "\n",
        "text_ans = final_cor(text_ans, flag_dot)\n",
        "\n",
        "# print('\\n\\nИсправленный текст:\\n', text_ans)\n",
        "\n",
        "\n",
        "\n",
        "precision, recall, F_measure, count_of_mistakes = evaluetion(standard, text, text_ans)\n",
        "\n",
        "print('\\nPrecision:', precision, 'Recall:', recall, 'F-мера:', F_measure, '\\nИсправлено:', mist - count_of_mistakes, 'из:', mist)\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Precision: 0.9722031571722718 Recall: 0.9326748971193416 F-мера: 0.9520289002772411 \n",
            "Исправлено: 7371 из: 7942\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiHT7Ad-C2LE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZV5l5KCQkkzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}